---
title: "FVCOM Monthly Extract"
author: "Matt Dzaugis"
date: "10/18/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)


climatology_fun <- function(x, var_name){
  var_name <- ensym(var_name)
  clim <- x %>% 
    filter(Year >= 1990,
           Year <= 2020) %>% 
    group_by(Month, stat_area) %>% 
    summarise(clim = mean(!!var_name),
              sd = sd(!!var_name), .groups = "drop")
  
  anom_name <- paste(var_name, "anom", sep = "_")
  
  x <- x %>% left_join(clim, by = c("Month", "stat_area")) %>% 
    mutate(!!anom_name := !!var_name - clim) %>% 
    dplyr::select(-clim, -sd)
}
```

## FVCOM monthly means

The code below extracts the monthly mean temperature and u and v currents from the FVCOM NECOFS data. The downloaded data are in the Res_Data folder. Data can also be accessed through the 

### Monthly mean temperature

```{r monthly_mean_temp}

nc_files <- list.files(gmRi::shared.path(group = "Res_Data", folder = "FVCOM_mon_means"), full.names = TRUE)

for(i in 1:length(nc_files)){
  if(i < 469){
    y <- ncdf4::nc_open(nc_files[[400]]) # get a mesh with lat and lon values (for some reason the early files have 0 for lat and lon)
    latlons <- dplyr::left_join(fvcom::fvcom_nodes(y, what = 'lonlat'), 
                     fvcom::fvcom_nodes(y, what = 'xy'), by = "node")
    nc_close(y)
    x <- ncdf4::nc_open(nc_files[[i]])
  }
  if(i >= 469){
    x <- ncdf4::nc_open(nc_files[[i]])
    latlons <- dplyr::left_join(fvcom::fvcom_nodes(x, what = 'lonlat'), 
                     fvcom::fvcom_nodes(x, what = 'xy'), by = "node")
  }
  name <- str_replace(basename(nc_files[[i]]), ".nc", "")
  sigLevel <- x$dim$siglay$len
  sur_temp <- ncdf4::ncvar_get(x, varid = "temp")[,1]
  bot_temp <- ncdf4::ncvar_get(x, varid = "temp")[,sigLevel]
  h <- ncdf4::ncvar_get(x, varid = "h")
  lon <- ncdf4::ncvar_get(x, varid = "x")
  lat <- ncdf4::ncvar_get(x, varid = "y")
  tt <- tibble("sur_temp" = sur_temp, "bot_temp" = bot_temp, "depth" = h, "x" = lon, "y" = lat) %>% 
    left_join(latlons, by = c("x", "y"))
  write_csv(tt, paste0("Intermediate_temp_data/", name,".csv"))
  nc_close(x)
  print(name)
}

lob_zones_extract <- function(df){
  numCores <- parallel::detectCores()
  doParallel::registerDoParallel(numCores)
  lob_zone_temps <-  foreach (i=1:length(lobzone$ZONEID), .combine = rbind, .errorhandling = "remove") %dopar% {
    name <- lobzone@data[["ZONEID"]][i]
    zone <- lobzone@polygons[[i]]@Polygons[[1]]@coords
    zone <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(zone)), ID =1)))
    raster::crs(zone) <- sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
    values <- sp::over(point_df, zone)
    values <- point_df[values == 1 & !is.na(values),]
    values <- as.data.frame(values)
    values <- values %>% summarise(sur_zone_avg = mean(sur_temp, na.rm =TRUE),
                                                   bot_zone_avg = mean(bot_temp, na.rm = TRUE),
                                                   zone_avg_depth = mean(depth, na.rm = TRUE),
                                                   zone = name) %>% ungroup()
    values
  }
  doParallel::stopImplicitCluster()
  return(lob_zone_temps)
}

stat_area_extract <- function(df){
  doParallel::registerDoParallel(3)
  stat_area_temps <-  foreach (j=c(34,35,36), .combine = rbind, .errorhandling = "remove") %dopar% {
    name <- statarea@data[["Id"]][j]
    zone <- statarea@polygons[[j]]@Polygons[[1]]@coords
    zone <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(zone)), ID =1)))
    raster::crs(zone) <- sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
    values <- sp::over(point_df, zone)
    values <- point_df[values == 1 & !is.na(values),]
    values <- as.data.frame(values)
    values <- values %>% 
      mutate(lon = round(lon, 1),
             lat = round(lat, 1)) %>% 
      group_by(lon, lat) %>% summarise(sur_temp = mean(sur_temp, na.rm =TRUE),
                                                   bot_temp = mean(bot_temp, na.rm = TRUE),
                                                   depth = mean(depth, na.rm = TRUE),
                                                   stat_area = name, .groups = "drop")

    values
  }
  doParallel::stopImplicitCluster()
  return(stat_area_temps)
}

csv_files <- list.files(here::here("Intermediate_temp_data"), full.names = TRUE)

statarea <- rgdal::readOGR(paste0(gmRi::shared.path(group = "Res_Data", folder = "Shapefiles/Statistical_Areas"), "Statistical_Areas_2010_withnames.shp"))

library(doParallel)
Zonal_avgs <- list()
for(i in 1:length(csv_files)){
  df <- read_csv(csv_files[[i]])
  name <- str_replace(basename(csv_files[[i]]), ".csv", "")
  xy <- df[,c("lon","lat")]
  point_df <- sp::SpatialPointsDataFrame(coords = xy, data = df,
                               proj4string = sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))
  Zonal_avgs[[name]] <- stat_area_extract(point_df)
  print(name)
}

stat_area_avgs <- bind_rows(Zonal_avgs, .id = "Date") %>% 
  mutate("Year" = as.numeric(str_sub(Date, 1, 4)),
         "Month" = as.numeric(str_sub(Date, 5,6))) %>% 
  dplyr::select(-Date)

# get anomalies
tt <- climatology_fun(stat_area_avgs, "sur_temp")
yy <- climatology_fun(stat_area_avgs, "bot_temp")

FVCOM_temp_anoms <- left_join(tt, yy)

write_csv(FVCOM_temp_anoms, here::here("Indicators/FVCOM_stat_area_temps.csv"))

```

### Monthly mean salinity

```{r monthly_mean_salinity}

nc_files <- list.files(gmRi::shared.path(group = "Res_Data", folder = "FVCOM_mon_means"), full.names = TRUE)

for(i in 1:length(nc_files)){
  if(i < 469){
    y <- ncdf4::nc_open(nc_files[[400]]) # get a mesh with lat and lon values (for some reason the early files have 0 for lat and lon)
    latlons <- dplyr::left_join(fvcom::fvcom_nodes(y, what = 'lonlat'), 
                     fvcom::fvcom_nodes(y, what = 'xy'), by = "node")
    nc_close(y)
    x <- ncdf4::nc_open(nc_files[[i]])
  }
  if(i >= 469){
    x <- ncdf4::nc_open(nc_files[[i]])
    latlons <- dplyr::left_join(fvcom::fvcom_nodes(x, what = 'lonlat'), 
                     fvcom::fvcom_nodes(x, what = 'xy'), by = "node")
  }
  name <- str_replace(basename(nc_files[[i]]), ".nc", "")
  sigLevel <- x$dim$siglay$len
  sur_sal <- ncdf4::ncvar_get(x, varid = "salinity")[,1]
  bot_sal <- ncdf4::ncvar_get(x, varid = "salinity")[,sigLevel]
  h <- ncdf4::ncvar_get(x, varid = "h")
  lon <- ncdf4::ncvar_get(x, varid = "x")
  lat <- ncdf4::ncvar_get(x, varid = "y")
  tt <- tibble("sur_sal" = sur_sal, "bot_sal" = bot_sal, "depth" = h, "x" = lon, "y" = lat) %>% 
    left_join(latlons, by = c("x", "y"))
  write_csv(tt, paste0("Intermediate_sal_data/", name,".csv"))
  nc_close(x)
  print(name)
}

lob_zones_extract <- function(df){
  numCores <- parallel::detectCores()
  doParallel::registerDoParallel(numCores)
  lob_zone_temps <-  foreach (i=1:length(lobzone$ZONEID), .combine = rbind, .errorhandling = "remove") %dopar% {
    name <- lobzone@data[["ZONEID"]][i]
    zone <- lobzone@polygons[[i]]@Polygons[[1]]@coords
    zone <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(zone)), ID =1)))
    raster::crs(zone) <- sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
    values <- sp::over(point_df, zone)
    values <- point_df[values == 1 & !is.na(values),]
    values <- as.data.frame(values)
    values <- values %>% summarise(sur_zone_avg = mean(sur_sal, na.rm =TRUE),
                                                   bot_zone_avg = mean(bot_sal, na.rm = TRUE),
                                                   zone_avg_depth = mean(depth, na.rm = TRUE),
                                                   zone = name) %>% ungroup()
    values
  }
  doParallel::stopImplicitCluster()
  return(lob_zone_temps)
}

stat_area_extract <- function(df){
  doParallel::registerDoParallel(3)
  stat_area_temps <-  foreach (j=c(34,35,36), .combine = rbind, .errorhandling = "remove") %dopar% {
    name <- statarea@data[["Id"]][j]
    zone <- statarea@polygons[[j]]@Polygons[[1]]@coords
    zone <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(zone)), ID =1)))
    raster::crs(zone) <- sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
    values <- sp::over(point_df, zone)
    values <- point_df[values == 1 & !is.na(values),]
    values <- as.data.frame(values)
    values <- values %>% 
      mutate(lon = round(lon, 1),
             lat = round(lat, 1)) %>% 
      group_by(lon, lat) %>% summarise(sur_sal = mean(sur_sal, na.rm =TRUE),
                                                   bot_sal = mean(bot_sal, na.rm = TRUE),
                                                   depth = mean(depth, na.rm = TRUE),
                                                   stat_area = name, .groups = "drop")
    values
  }
  doParallel::stopImplicitCluster()
  return(stat_area_temps)
}

csv_files <- list.files(here::here("Intermediate_sal_data"), full.names = TRUE)

statarea <- rgdal::readOGR(paste0(gmRi::shared.path(group = "Res_Data", folder = "Shapefiles/Statistical_Areas"), "Statistical_Areas_2010_withnames.shp"))

library(doParallel)
Zonal_avgs <- list()
for(i in 1:length(csv_files)){
  df <- read_csv(csv_files[[i]])
  name <- str_replace(basename(csv_files[[i]]), ".csv", "")
  xy <- df[,c("lon","lat")]
  point_df <- sp::SpatialPointsDataFrame(coords = xy, data = df,
                               proj4string = sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))
  Zonal_avgs[[name]] <- stat_area_extract(point_df)
  print(name)
}

stat_area_avgs <- bind_rows(Zonal_avgs, .id = "Date") %>% 
  mutate("Year" = as.numeric(str_sub(Date, 1, 4)),
         "Month" = as.numeric(str_sub(Date, 5,6))) %>% 
  dplyr::select(-Date)

# get anomalies
tt <- climatology_fun(stat_area_avgs, "sur_sal")
yy <- climatology_fun(stat_area_avgs, "bot_sal")

FVCOM_sal_anoms <- left_join(tt, yy)

write_csv(FVCOM_sal_anoms, here::here("Indicators/FVCOM_stat_area_sal.csv"))

```

### Monthly mean current strength and direction

```{r monthly_mean_currents}
nc_files <- list.files(gmRi::shared.path(group = "Res_Data", folder = "FVCOM_mon_means"), full.names = TRUE)

x <- ncdf4::nc_open(nc_files[[400]]) # get a mesh with lat and lon values (for some reason the early files have 0 for lat and lon)
elems <- dplyr::left_join(fvcom::fvcom_elems(x, what = 'lonlat'), 
                 fvcom::fvcom_elems(x, what = 'xy'), by = "elem")

for(i in 1:length(nc_files)){
  x <- ncdf4::nc_open(nc_files[[i]])
  name <- str_replace(basename(nc_files[[i]]), ".nc", "")
  sigLevel <- x$dim$siglay$len
  sur_cur_u <- ncdf4::ncvar_get(x, varid = "u")[,1]
  vert_cur_u <- ncdf4::ncvar_get(x, varid = "ua")
  sur_cur_v <- ncdf4::ncvar_get(x, varid = "v")[,1]
  vert_cur_v <- ncdf4::ncvar_get(x, varid = "va")
  
  z1 <- as.vector(sur_cur_u) %>% data.frame("u" = .)
  
  currents <- z1 %>% add_column(., "v" = as.vector(sur_cur_v)) %>% 
    add_column(., "u_vert" = vert_cur_u) %>% 
    add_column(., "v_vert" = vert_cur_v) %>% 
    bind_cols(., elems) 
  
  write_csv(currents, paste0("Intermediate_cur_data/", name,".csv"))
  ncdf4::nc_close(x)
  print(name)
}

extract_data <- function(df, shape){
  xy <- df %>% ungroup() %>% dplyr::select(lon, lat)
  point_df <- sp::SpatialPointsDataFrame(coords = xy, data = df,
                               proj4string = sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"))
  raster::crs(shape) <- sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
  zone_cur <- sp::over(point_df, shape)
  zone_cur <- point_df[zone_cur == 1 & !is.na(zone_cur),]
  zone_cur <- as.data.frame(zone_cur)
  return(zone_cur)}

mcc_turnoff_shp <- rgdal::readOGR(here::here("data/Shapefiles/MCC_turnoff/MCC_turnoff.shp"))
csv_files <- list.files(here::here("Intermediate_cur_data"), full.names = TRUE)

cur_list <- list()
for(i in 1:length(csv_files)){
  df <- read_csv(csv_files[[i]])
  name <- str_replace(basename(csv_files[[i]]), ".csv", "")
  zone <- mcc_turnoff_shp@polygons[[1]]@Polygons[[1]]@coords
  zone <- sp::SpatialPolygons(list(sp::Polygons(list(sp::Polygon(zone)), ID =1)))
  raster::crs(zone) <- sp::CRS("+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0")
  cur_list[[name]] <- extract_data(df, zone)
  print(name)
}

cur_df <- bind_rows(cur_list, .id = "Date") %>% 
  mutate("Year" = as.numeric(str_sub(Date, 1, 4)), "Month" = as.numeric(str_sub(Date, 5,6))) %>% 
  dplyr::select(-Date)

mcc_turnoff_subset <- cur_df %>% 
  mutate(lon = round(lon, 1), lat = round(lat, 1)) %>%
  group_by(lat, lon, Year, Month) %>% 
  summarise(u = mean(u), 
            v = mean(v), 
            u_vert = mean(u_vert), 
            v_vert = mean(v_vert), .groups = "drop") %>% 
  mutate(Date = as.Date(paste(Year, Month, "01", sep = "-")))

write_csv(mcc_turnoff_subset, here::here("Indicators/mcc_turnoff_subset.csv"))

```

